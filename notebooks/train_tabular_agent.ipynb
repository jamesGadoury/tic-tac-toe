{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39110590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Self\n",
    "from functools import cache\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a718921",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeBoard:\n",
    "    EMPTY_CELL, FIRST_PLAYER_CELL, SECOND_PLAYER_CELL = 0, 1, 2\n",
    "\n",
    "    WIN_COMBOS = (\n",
    "        (0, 1, 2),\n",
    "        (3, 4, 5),\n",
    "        (6, 7, 8),\n",
    "        (0, 3, 6),\n",
    "        (1, 4, 7),\n",
    "        (2, 5, 8),\n",
    "        (0, 4, 8),\n",
    "        (2, 4, 6),\n",
    "    )\n",
    "\n",
    "    WIN_ARRAY = np.array(WIN_COMBOS, dtype=int)  # shape (8,3)\n",
    "\n",
    "    def __init__(self):\n",
    "        # NOTE: using EMPTY in case that value changes\n",
    "        #       for whatever reason, but obv if EMPTY=0\n",
    "        #       then this is equivalent to np.zeros((9,))\n",
    "        self._state = np.ones((9,)) * TicTacToeBoard.EMPTY_CELL\n",
    "\n",
    "    @property\n",
    "    def state(self) -> tuple[int, ...]:\n",
    "        return tuple(self._state.tolist())\n",
    "\n",
    "    # NOTE: __eq__ and __hash__ need to be implemented for\n",
    "    #       cache functionality to work \n",
    "    def __eq__(self, other: Self):\n",
    "        return (self._state == other._state).all()\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.state)\n",
    "\n",
    "    @cache\n",
    "    def player_to_move(self) -> int:\n",
    "        return TicTacToeBoard.FIRST_PLAYER_CELL if len(self.available_cell_indices()) % 2 == 1 else TicTacToeBoard.SECOND_PLAYER_CELL\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self._state *= TicTacToeBoard.EMPTY_CELL\n",
    "\n",
    "    @cache\n",
    "    def available_cell_indices(self) -> tuple[int, ...]:\n",
    "        return tuple((self._state == TicTacToeBoard.EMPTY_CELL).nonzero()[0].tolist())\n",
    "    \n",
    "    @cache\n",
    "    def terminated(self) -> bool:\n",
    "        return self.tied() or self.first_player_won() or self.second_player_won()\n",
    "\n",
    "    @cache\n",
    "    def tied(self) -> bool:\n",
    "        return (self._state != TicTacToeBoard.EMPTY_CELL).all()\n",
    "\n",
    "    @cache\n",
    "    def first_player_won(self) -> bool:\n",
    "        return self.player_won(player=TicTacToeBoard.FIRST_PLAYER_CELL)\n",
    "\n",
    "    @cache\n",
    "    def second_player_won(self) -> bool:\n",
    "        return self.player_won(player=TicTacToeBoard.SECOND_PLAYER_CELL)\n",
    "\n",
    "    @cache\n",
    "    def player_won(self, player: int) -> bool:\n",
    "        cells = self._state[TicTacToeBoard.WIN_ARRAY]\n",
    "        return bool(np.any(np.all(cells == player, axis=1)))\n",
    "\n",
    "    @cache\n",
    "    def transition(self, idx: int) -> Self:\n",
    "        if self.terminated():\n",
    "            raise RuntimeError(\"move attempted on completed game\")\n",
    "\n",
    "        if idx not in self.available_cell_indices():\n",
    "            raise RuntimeError(\"illegal move\")\n",
    "        new_board = self.__class__()\n",
    "        new_board._state = self._state.copy()\n",
    "        new_board._state[idx] = self.player_to_move()\n",
    "        return new_board\n",
    "    \n",
    "    def display(self):\n",
    "        keys = {TicTacToeBoard.FIRST_PLAYER_CELL: \"X\", TicTacToeBoard.SECOND_PLAYER_CELL: \"O\", TicTacToeBoard.EMPTY_CELL: \"_\"}\n",
    "        for i in range(0, 9, 3):\n",
    "            print(f\"{keys[self._state[i]]} {keys[self._state[i+1]]} {keys[self._state[i+2]]}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9901773",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_CELL, OPPONENT_CELL, EMPTY_CELL = \"A\", \"O\", \"_\"\n",
    "# NOTE: ties are considered same as default value\n",
    "WON_VALUE, LOST_VALUE, DEFAULT_VALUE = 1.0, 0.0, 0.5\n",
    "\n",
    "@cache\n",
    "def serialize_board(board: TicTacToeBoard, player: int) -> str:\n",
    "    return \"\".join([\n",
    "        EMPTY_CELL if c == TicTacToeBoard.EMPTY_CELL\n",
    "        else (AGENT_CELL if c == player else OPPONENT_CELL)\n",
    "        for c in board.state\n",
    "    ])\n",
    "\n",
    "\n",
    "def populate_value_if_non_existent(value_table: dict, board: TicTacToeBoard, player: int):\n",
    "    key = serialize_board(board, player=player)\n",
    "    if key not in value_table:\n",
    "        value_table[key] = DEFAULT_VALUE\n",
    "    if board.first_player_won():\n",
    "        value_table[key] = (WON_VALUE if player == TicTacToeBoard.FIRST_PLAYER_CELL else LOST_VALUE)\n",
    "    elif board.second_player_won():\n",
    "        value_table[key] = (WON_VALUE if player == TicTacToeBoard.SECOND_PLAYER_CELL else LOST_VALUE)\n",
    "\n",
    "def get_value(value_table: dict, board: TicTacToeBoard, player: int):\n",
    "    populate_value_if_non_existent(value_table, board, player)\n",
    "    return value_table[serialize_board(board, player)]\n",
    "\n",
    "def sample_policy(value_table: dict, board: TicTacToeBoard, player: int, eps: float = 0.0) -> tuple[float, bool]:\n",
    "    \"\"\"Returns action sampled from policy and bool indicating whether or not a greedy move was selected\"\"\"\n",
    "    populate_value_if_non_existent(value_table=value_table, board=board, player=player)\n",
    "    possible_plays: list[tuple[int, float]] = []\n",
    "    for idx in board.available_cell_indices():\n",
    "        next_board = board.transition(idx)\n",
    "        populate_value_if_non_existent(value_table=value_table, board=next_board, player=player)\n",
    "        possible_plays.append((idx, get_value(value_table, next_board, player)))\n",
    "\n",
    "    greedy_idx, _ = max(possible_plays, key=lambda t: t[1])\n",
    "    if len(possible_plays) > 1 and np.random.rand() < eps:\n",
    "        return np.random.choice([idx for idx, _ in possible_plays if idx != greedy_idx]), False\n",
    "\n",
    "    return greedy_idx, True\n",
    "\n",
    "def update_value(value_table: dict, last_board: TicTacToeBoard, current_board: TicTacToeBoard, player: int, learning_rate: float):\n",
    "    key_last = serialize_board(last_board, player)\n",
    "    # these two calls will auto‐create missing entries \n",
    "    v_last = get_value(value_table, last_board, player)\n",
    "    v_curr = get_value(value_table, current_board, player)\n",
    "    value_table[key_last] = v_last + learning_rate * (v_curr - v_last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1bb5ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(value_table, path):\n",
    "    # dump keys as strings for JSON\n",
    "    export = {\"\".join(str(x) for x in k): v for k, v in value_table.items()}\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(export, f)\n",
    "\n",
    "def load(path):\n",
    "    raw = json.load(open(path, \"r\"))\n",
    "    return {tuple(int(c) for c in k): v for k, v in raw.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8ed6347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: self play loop, agent plays both first and second player, changing its\n",
    "#       role after each turn\n",
    "\n",
    "FIRST_PLAYER_WON=\"first player won\"\n",
    "SECOND_PLAYER_WON=\"second player won\"\n",
    "TIE=\"tie\"\n",
    "\n",
    "# TODO: this is so gross lol make it not gross pls \n",
    "# TODO: replace debug with using logs instead of prints so we can set log level at top of notebook\n",
    "def train(value_table, n_episodes: int, learning_rate: float, epsilon: float, debug=False) -> dict:\n",
    "    outcomes = {FIRST_PLAYER_WON: 0, SECOND_PLAYER_WON: 0, TIE: 0}\n",
    "    for i in tqdm(range(n_episodes)):\n",
    "        current = TicTacToeBoard()\n",
    "        p1_greedy = False\n",
    "        p2_greedy = False\n",
    "        last_p1_turn = None\n",
    "        last_p2_turn = None\n",
    "\n",
    "        while True:\n",
    "            action, p1_greedy = sample_policy(value_table=value_table, board=current, player=TicTacToeBoard.FIRST_PLAYER_CELL, eps=epsilon)\n",
    "            last_p1_turn = current\n",
    "\n",
    "            current = current.transition(action)\n",
    "            if p2_greedy:\n",
    "                update_value(value_table=value_table, last_board=last_p2_turn, current_board=current, player=TicTacToeBoard.SECOND_PLAYER_CELL, learning_rate=learning_rate)\n",
    "                p2_greedy = False\n",
    "\n",
    "            if debug:\n",
    "                current.display()\n",
    "\n",
    "            if current.terminated():\n",
    "                break\n",
    "\n",
    "            action, p2_greedy = sample_policy(value_table=value_table, board=current, player=TicTacToeBoard.SECOND_PLAYER_CELL, eps=epsilon)\n",
    "            last_p2_turn = current\n",
    "\n",
    "            current = current.transition(action)\n",
    "            if p1_greedy:\n",
    "                update_value(value_table=value_table, last_board=last_p1_turn, current_board=current, player=TicTacToeBoard.FIRST_PLAYER_CELL, learning_rate=learning_rate)\n",
    "                p1_greedy = False\n",
    "\n",
    "            if debug:\n",
    "                current.display()\n",
    "\n",
    "            if current.terminated():\n",
    "                break\n",
    "\n",
    "        if p1_greedy:\n",
    "            update_value(value_table=value_table, last_board=last_p1_turn, current_board=current, player=TicTacToeBoard.FIRST_PLAYER_CELL, learning_rate=learning_rate)\n",
    "        if p2_greedy:\n",
    "            update_value(value_table=value_table, last_board=last_p2_turn, current_board=current, player=TicTacToeBoard.SECOND_PLAYER_CELL, learning_rate=learning_rate)\n",
    "\n",
    "        assert current.terminated(), \"should be terminated at end of episode\"\n",
    "        outcomes[FIRST_PLAYER_WON if current.first_player_won() else SECOND_PLAYER_WON if current.second_player_won() else TIE] += 1\n",
    "\n",
    "    return outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbf2edd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [04:22<00:00, 3806.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'first player won': 852998, 'second player won': 133820, 'tie': 13182}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "value_table = {}\n",
    "train(value_table=value_table, n_episodes=1000000, learning_rate=0.05, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "356f9138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_eval_policy(board: TicTacToeBoard) -> int:\n",
    "    player_to_move = board.player_to_move()\n",
    "    avail = board.available_cell_indices()\n",
    "    if len(avail) == 0:\n",
    "        raise ValueError(\"No moves left\")\n",
    "    for c in avail:\n",
    "        if board.transition(c).player_won(player_to_move):\n",
    "            return c\n",
    "    return int(np.random.choice(avail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27bdbcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(value_table, n_episodes, player):\n",
    "    outcomes = {FIRST_PLAYER_WON: 0, SECOND_PLAYER_WON: 0, TIE: 0}\n",
    "    # TODO: same as above training loop, make loop cleaner using cycle to switch through current player or somethign\n",
    "    for i in tqdm(range(n_episodes)):\n",
    "        current = TicTacToeBoard()\n",
    "\n",
    "        while True:\n",
    "            action = None\n",
    "            if player == TicTacToeBoard.FIRST_PLAYER_CELL:\n",
    "                action, _ = sample_policy(value_table=value_table, board=current, player=player, eps=0.0)\n",
    "            else:\n",
    "                action = sample_eval_policy(board=current)\n",
    "            current = current.transition(action)\n",
    "\n",
    "            # TODO: change to use logging\n",
    "            # current.display()\n",
    "\n",
    "            if current.terminated():\n",
    "                break\n",
    "\n",
    "\n",
    "            if player == TicTacToeBoard.SECOND_PLAYER_CELL:\n",
    "                action, _ = sample_policy(value_table=value_table, board=current, player=player, eps=0.0)\n",
    "            else:\n",
    "                action = sample_eval_policy(board=current)\n",
    "            current.transition(action)\n",
    "            # TODO: change to use logging\n",
    "            # current.display()\n",
    "            if current.terminated():\n",
    "                break\n",
    "        \n",
    "        assert current.terminated(), \"should be terminated at end of episode\"\n",
    "        outcomes[FIRST_PLAYER_WON if current.first_player_won() else SECOND_PLAYER_WON if current.second_player_won() else TIE] += 1\n",
    "\n",
    "    return outcomes\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8417eae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 4623.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'first player won': 10000, 'second player won': 0, 'tie': 0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_policy(value_table=value_table, n_episodes=10000, player=TicTacToeBoard.FIRST_PLAYER_CELL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3983bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(value_table=value_table, path=\"table.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfeefe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_to_ui_format(value_table):\n",
    "    return {\"\".join([\"1\" if c == AGENT_CELL else \"2\" if c == OPPONENT_CELL else \"0\" for c in s]): v for s, v in value_table.items()}\n",
    "\n",
    "save(value_table=serialize_to_ui_format(value_table=value_table), path=\"ui_value_table.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f302e54e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
