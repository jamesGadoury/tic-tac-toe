{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39110590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Self\n",
    "from functools import cache\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a718921",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeBoard:\n",
    "    EMPTY_CELL, FIRST_PLAYER_CELL, SECOND_PLAYER_CELL = 0, 1, 2\n",
    "\n",
    "    WIN_COMBOS = (\n",
    "        (0, 1, 2),\n",
    "        (3, 4, 5),\n",
    "        (6, 7, 8),\n",
    "        (0, 3, 6),\n",
    "        (1, 4, 7),\n",
    "        (2, 5, 8),\n",
    "        (0, 4, 8),\n",
    "        (2, 4, 6),\n",
    "    )\n",
    "\n",
    "    WIN_ARRAY = np.array(WIN_COMBOS, dtype=int)  # shape (8,3)\n",
    "\n",
    "    def __init__(self):\n",
    "        # NOTE: using EMPTY in case that value changes\n",
    "        #       for whatever reason, but obv if EMPTY=0\n",
    "        #       then this is equivalent to np.zeros((9,))\n",
    "        self._state = np.ones((9,)) * TicTacToeBoard.EMPTY_CELL\n",
    "\n",
    "    @property\n",
    "    def state(self) -> tuple[int, ...]:\n",
    "        return tuple(self._state.tolist())\n",
    "\n",
    "    # NOTE: __eq__ and __hash__ need to be implemented for\n",
    "    #       cache functionality to work \n",
    "    def __eq__(self, other: Self):\n",
    "        return (self._state == other._state).all()\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.state)\n",
    "\n",
    "    @cache\n",
    "    def player_to_move(self) -> int:\n",
    "        return TicTacToeBoard.FIRST_PLAYER_CELL if len(self.available_cell_indices()) % 2 == 1 else TicTacToeBoard.SECOND_PLAYER_CELL\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self._state *= TicTacToeBoard.EMPTY_CELL\n",
    "\n",
    "    @cache\n",
    "    def available_cell_indices(self) -> tuple[int, ...]:\n",
    "        return tuple((self._state == TicTacToeBoard.EMPTY_CELL).nonzero()[0].tolist())\n",
    "    \n",
    "    @cache\n",
    "    def terminated(self) -> bool:\n",
    "        return self.tied() or self.first_player_won() or self.second_player_won()\n",
    "\n",
    "    @cache\n",
    "    def tied(self) -> bool:\n",
    "        return (self._state != TicTacToeBoard.EMPTY_CELL).all()\n",
    "\n",
    "    @cache\n",
    "    def first_player_won(self) -> bool:\n",
    "        return self.player_won(player=TicTacToeBoard.FIRST_PLAYER_CELL)\n",
    "\n",
    "    @cache\n",
    "    def second_player_won(self) -> bool:\n",
    "        return self.player_won(player=TicTacToeBoard.SECOND_PLAYER_CELL)\n",
    "\n",
    "    @cache\n",
    "    def player_won(self, player: int) -> bool:\n",
    "        cells = self._state[TicTacToeBoard.WIN_ARRAY]\n",
    "        return bool(np.any(np.all(cells == player, axis=1)))\n",
    "\n",
    "    @cache\n",
    "    def transition(self, idx: int) -> Self:\n",
    "        if self.terminated():\n",
    "            raise RuntimeError(\"move attempted on completed game\")\n",
    "\n",
    "        if idx not in self.available_cell_indices():\n",
    "            raise RuntimeError(\"illegal move\")\n",
    "        new_board = self.__class__()\n",
    "        new_board._state = self._state.copy()\n",
    "        new_board._state[idx] = self.player_to_move()\n",
    "        return new_board\n",
    "    \n",
    "    def display(self):\n",
    "        keys = {TicTacToeBoard.FIRST_PLAYER_CELL: \"X\", TicTacToeBoard.SECOND_PLAYER_CELL: \"O\", TicTacToeBoard.EMPTY_CELL: \"_\"}\n",
    "        for i in range(0, 9, 3):\n",
    "            print(f\"{keys[self._state[i]]} {keys[self._state[i+1]]} {keys[self._state[i+2]]}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9901773",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_CELL, OPPONENT_CELL, EMPTY_CELL = \"A\", \"O\", \"_\"\n",
    "# NOTE: ties are considered same as default value\n",
    "WON_VALUE, LOST_VALUE, DEFAULT_VALUE = 1.0, 0.0, 0.5\n",
    "\n",
    "@cache\n",
    "def serialize_board(board: TicTacToeBoard, player: int) -> str:\n",
    "    return \"\".join([\n",
    "        EMPTY_CELL if c == TicTacToeBoard.EMPTY_CELL\n",
    "        else (AGENT_CELL if c == player else OPPONENT_CELL)\n",
    "        for c in board.state\n",
    "    ])\n",
    "\n",
    "\n",
    "def populate_value_if_non_existent(value_table: dict, board: TicTacToeBoard, player: int):\n",
    "    key = serialize_board(board, player=player)\n",
    "    if key not in value_table:\n",
    "        value_table[key] = DEFAULT_VALUE\n",
    "    if board.first_player_won():\n",
    "        value_table[key] = (WON_VALUE if player == TicTacToeBoard.FIRST_PLAYER_CELL else LOST_VALUE)\n",
    "    elif board.second_player_won():\n",
    "        value_table[key] = (WON_VALUE if player == TicTacToeBoard.SECOND_PLAYER_CELL else LOST_VALUE)\n",
    "\n",
    "def get_value(value_table: dict, board: TicTacToeBoard, player: int):\n",
    "    populate_value_if_non_existent(value_table, board, player)\n",
    "    return value_table[serialize_board(board, player)]\n",
    "\n",
    "def sample_policy(\n",
    "    value_table: dict,\n",
    "    board: TicTacToeBoard,\n",
    "    player: int,\n",
    "    eps: float = 0.0\n",
    ") -> tuple[int, bool]:\n",
    "    \"\"\"\n",
    "    Greedy (with eps) over the VALUE of the next *decision* state,\n",
    "    i.e. after you move AND the opponent has replied—without recursion.\n",
    "    \"\"\"\n",
    "    # identify opponent\n",
    "    other = (TicTacToeBoard.SECOND_PLAYER_CELL\n",
    "             if player == TicTacToeBoard.FIRST_PLAYER_CELL\n",
    "             else TicTacToeBoard.FIRST_PLAYER_CELL)\n",
    "\n",
    "    # make sure current board is initialized\n",
    "    populate_value_if_non_existent(value_table, board, player)\n",
    "\n",
    "    candidates: list[tuple[int, float]] = []\n",
    "    for idx in board.available_cell_indices():\n",
    "        # 1) simulate your move\n",
    "        b1 = board.transition(idx)\n",
    "\n",
    "        # 2) simulate opponent's greedy reply (no eps) at b1\n",
    "        if not b1.terminated():\n",
    "            # ensure b1 is in the table for opponent's POV\n",
    "            populate_value_if_non_existent(value_table, b1, other)\n",
    "\n",
    "            # find opponent's best reply\n",
    "            best_val = None\n",
    "            best_opp_idx = None\n",
    "            for opp_idx in b1.available_cell_indices():\n",
    "                b2_opp = b1.transition(opp_idx)\n",
    "                populate_value_if_non_existent(value_table, b2_opp, other)\n",
    "                v_opp = get_value(value_table, b2_opp, other)\n",
    "                if best_val is None or v_opp > best_val:\n",
    "                    best_val = v_opp\n",
    "                    best_opp_idx = opp_idx\n",
    "            # apply that reply\n",
    "            b2 = b1.transition(best_opp_idx)\n",
    "        else:\n",
    "            # game ended on your move\n",
    "            b2 = b1\n",
    "\n",
    "        # 3) now b2 is the board at your next turn (or terminal)\n",
    "        populate_value_if_non_existent(value_table, b2, player)\n",
    "        v_next = get_value(value_table, b2, player)\n",
    "        candidates.append((idx, v_next))\n",
    "\n",
    "    # 4) pick greedy action\n",
    "    greedy_idx, _ = max(candidates, key=lambda x: x[1])\n",
    "\n",
    "    # 5) epsilon‐exploration\n",
    "    if len(candidates) > 1 and np.random.rand() < eps:\n",
    "        other_idxs = [i for i, _ in candidates if i != greedy_idx]\n",
    "        return np.random.choice(other_idxs), False\n",
    "\n",
    "    return greedy_idx, True\n",
    "\n",
    "\n",
    "\n",
    "def update_value(value_table: dict, last_board: TicTacToeBoard, current_board: TicTacToeBoard, player: int, learning_rate: float):\n",
    "    key_last = serialize_board(last_board, player)\n",
    "    # these two calls will auto‐create missing entries \n",
    "    v_last = get_value(value_table, last_board, player)\n",
    "    v_curr = get_value(value_table, current_board, player)\n",
    "    value_table[key_last] = v_last + learning_rate * (v_curr - v_last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1bb5ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(value_table, path):\n",
    "    # dump keys as strings for JSON\n",
    "    export = {\"\".join(str(x) for x in k): v for k, v in value_table.items()}\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(export, f)\n",
    "\n",
    "def load(path):\n",
    "    raw = json.load(open(path, \"r\"))\n",
    "    return {tuple(int(c) for c in k): v for k, v in raw.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8ed6347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: self play loop, agent plays both first and second player, changing its\n",
    "#       role after each turn\n",
    "\n",
    "FIRST_PLAYER_WON=\"first player won\"\n",
    "SECOND_PLAYER_WON=\"second player won\"\n",
    "TIE=\"tie\"\n",
    "\n",
    "def random_opponent(board: TicTacToeBoard, player: int) -> tuple[int, bool]:\n",
    "    \"\"\"Pick a random legal move. Always non‑greedy.\"\"\"\n",
    "    idx = np.random.choice(board.available_cell_indices())\n",
    "    return idx, False\n",
    "\n",
    "def train_selfplay(\n",
    "    value_table: dict,\n",
    "    n_episodes: int,\n",
    "    alpha0: float,\n",
    "    min_alpha: float,\n",
    "    alpha_decay: float,\n",
    "    eps0: float,\n",
    "    min_eps: float,\n",
    "    eps_decay: float,\n",
    "):\n",
    "    \"\"\"Self‐play: both sides use sample_policy (ε‐greedy), updating on every greedy move.\"\"\"\n",
    "    alpha, epsilon = alpha0, eps0\n",
    "\n",
    "    for _ in tqdm(range(n_episodes), desc=\"Self‑play\"):\n",
    "        board = TicTacToeBoard()\n",
    "        p1_last = p2_last = None\n",
    "        p1_greedy = p2_greedy = False\n",
    "\n",
    "        while not board.terminated():\n",
    "            # -- Player 1 --\n",
    "            idx, p1_greedy = sample_policy(value_table, board,\n",
    "                                           player=TicTacToeBoard.FIRST_PLAYER_CELL,\n",
    "                                           eps=epsilon)\n",
    "            p1_last, board = board, board.transition(idx)\n",
    "            if p2_greedy:\n",
    "                update_value(value_table, p2_last, board,\n",
    "                             TicTacToeBoard.SECOND_PLAYER_CELL, alpha)\n",
    "                p2_greedy = False\n",
    "            if board.terminated(): break\n",
    "\n",
    "            # -- Player 2 --\n",
    "            idx, p2_greedy = sample_policy(value_table, board,\n",
    "                                           player=TicTacToeBoard.SECOND_PLAYER_CELL,\n",
    "                                           eps=epsilon)\n",
    "            p2_last, board = board, board.transition(idx)\n",
    "            if p1_greedy:\n",
    "                update_value(value_table, p1_last, board,\n",
    "                             TicTacToeBoard.FIRST_PLAYER_CELL, alpha)\n",
    "                p1_greedy = False\n",
    "\n",
    "        # Final TD update for whoever moved last\n",
    "        if p1_greedy:\n",
    "            update_value(value_table, p1_last, board,\n",
    "                         TicTacToeBoard.FIRST_PLAYER_CELL, alpha)\n",
    "        if p2_greedy:\n",
    "            update_value(value_table, p2_last, board,\n",
    "                         TicTacToeBoard.SECOND_PLAYER_CELL, alpha)\n",
    "\n",
    "        # decay α and ε\n",
    "        alpha   = max(min_alpha, alpha * alpha_decay)\n",
    "        epsilon = max(min_eps,   epsilon * eps_decay)\n",
    "\n",
    "\n",
    "def train_against_random(\n",
    "    value_table: dict,\n",
    "    n_episodes: int,\n",
    "    alpha0: float,\n",
    "    min_alpha: float,\n",
    "    alpha_decay: float,\n",
    "    eps0: float,\n",
    "    min_eps: float,\n",
    "    eps_decay: float,\n",
    "    agent_plays_first: bool\n",
    "):\n",
    "    \"\"\"\n",
    "    Agent (with TD learning) vs random opponent.\n",
    "    If agent_plays_first=True, agent is Player 1; else agent is Player 2.\n",
    "    \"\"\"\n",
    "    alpha, epsilon = alpha0, eps0\n",
    "    agent = (TicTacToeBoard.FIRST_PLAYER_CELL\n",
    "             if agent_plays_first else TicTacToeBoard.SECOND_PLAYER_CELL)\n",
    "\n",
    "    for _ in tqdm(range(n_episodes),\n",
    "                  desc=f\"Agent vs Random ({'1st' if agent_plays_first else '2nd'})\"):\n",
    "        board = TicTacToeBoard()\n",
    "\n",
    "        last_board = None\n",
    "        last_greedy = False\n",
    "        current = TicTacToeBoard.FIRST_PLAYER_CELL\n",
    "\n",
    "        while not board.terminated():\n",
    "            if current == agent:\n",
    "                # agent’s turn: ε‑greedy on V\n",
    "                idx, last_greedy = sample_policy(value_table, board,\n",
    "                                                 player=current, eps=epsilon)\n",
    "                prev = board\n",
    "                board = board.transition(idx)\n",
    "                if last_greedy:\n",
    "                    update_value(value_table, prev, board, current, alpha)\n",
    "            else:\n",
    "                # random opponent\n",
    "                idx, _ = random_opponent(board, current)\n",
    "                board = board.transition(idx)\n",
    "                last_greedy = False\n",
    "\n",
    "            current = (TicTacToeBoard.SECOND_PLAYER_CELL\n",
    "                       if current == TicTacToeBoard.FIRST_PLAYER_CELL\n",
    "                       else TicTacToeBoard.FIRST_PLAYER_CELL)\n",
    "\n",
    "        # final update if agent moved last\n",
    "        if last_greedy and current != agent:\n",
    "            # `last_board` holds the board before agent’s last move\n",
    "            update_value(value_table, prev, board, agent, alpha)\n",
    "\n",
    "        # decay α and ε\n",
    "        alpha   = max(min_alpha, alpha * alpha_decay)\n",
    "        epsilon = max(min_eps,   epsilon * eps_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbf2edd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5648bf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self‑play: 100%|██████████| 100000/100000 [02:24<00:00, 690.88it/s]\n",
      "Agent vs Random (1st): 100%|██████████| 50000/50000 [00:56<00:00, 891.10it/s]\n",
      "Agent vs Random (2nd): 100%|██████████| 50000/50000 [01:01<00:00, 818.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "alpha0     = 0.1\n",
    "min_alpha  = 0.01\n",
    "alpha_decay= 0.995\n",
    "eps0       = 0.1\n",
    "min_eps    = 0.01\n",
    "eps_decay  = 0.995\n",
    "\n",
    "value_table = {}\n",
    "\n",
    "# 1) self‑play\n",
    "train_selfplay(\n",
    "    value_table=value_table,\n",
    "    n_episodes=100_000,\n",
    "    alpha0=alpha0,      min_alpha=min_alpha,\n",
    "    alpha_decay=alpha_decay,\n",
    "    eps0=eps0,          min_eps=min_eps,\n",
    "    eps_decay=eps_decay\n",
    ")\n",
    "\n",
    "# 2) agent as first vs random\n",
    "train_against_random(\n",
    "    value_table=value_table,\n",
    "    n_episodes=50_000,\n",
    "    alpha0=alpha0,      min_alpha=min_alpha,\n",
    "    alpha_decay=alpha_decay,\n",
    "    eps0=eps0,          min_eps=min_eps,\n",
    "    eps_decay=eps_decay,\n",
    "    agent_plays_first=True\n",
    ")\n",
    "\n",
    "# 3) agent as second vs random\n",
    "train_against_random(\n",
    "    value_table=value_table,\n",
    "    n_episodes=50_000,\n",
    "    alpha0=alpha0,      min_alpha=min_alpha,\n",
    "    alpha_decay=alpha_decay,\n",
    "    eps0=eps0,          min_eps=min_eps,\n",
    "    eps_decay=eps_decay,\n",
    "    agent_plays_first=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "691afd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:15<00:00, 662.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy evaluation (no eps): {'first player won': 0, 'second player won': 0, 'tie': 10000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def play_game(value_table: dict, eps: float = 0.0) -> str:\n",
    "    \"\"\"Plays one game greedily with fixed V-table and returns outcome.\"\"\"\n",
    "    board  = TicTacToeBoard()\n",
    "    player = TicTacToeBoard.FIRST_PLAYER_CELL\n",
    "\n",
    "    while not board.terminated():\n",
    "        # always greedy here\n",
    "        action, _ = sample_policy(\n",
    "            value_table, board,\n",
    "            player=player, eps=eps\n",
    "        )\n",
    "        board = board.transition(action)\n",
    "        # switch sides\n",
    "        player = (TicTacToeBoard.SECOND_PLAYER_CELL\n",
    "                  if player == TicTacToeBoard.FIRST_PLAYER_CELL\n",
    "                  else TicTacToeBoard.FIRST_PLAYER_CELL)\n",
    "\n",
    "    # return one of FIRST_PLAYER_WON, SECOND_PLAYER_WON, or TIE\n",
    "    if board.first_player_won():\n",
    "        return FIRST_PLAYER_WON\n",
    "    elif board.second_player_won():\n",
    "        return SECOND_PLAYER_WON\n",
    "    else:\n",
    "        return TIE\n",
    "\n",
    "\n",
    "# Run a no‑exploration evaluation\n",
    "results = {FIRST_PLAYER_WON: 0,\n",
    "           SECOND_PLAYER_WON: 0,\n",
    "           TIE: 0}\n",
    "\n",
    "for _ in tqdm(range(10_000)):\n",
    "    outcome = play_game(value_table, eps=0.0)\n",
    "    results[outcome] += 1\n",
    "\n",
    "print(\"Greedy evaluation (no eps):\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "356f9138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_eval_policy(board: TicTacToeBoard) -> int:\n",
    "    player_to_move = board.player_to_move()\n",
    "    avail = board.available_cell_indices()\n",
    "    if len(avail) == 0:\n",
    "        raise ValueError(\"No moves left\")\n",
    "    for c in avail:\n",
    "        if board.transition(c).player_won(player_to_move):\n",
    "            return c\n",
    "    return int(np.random.choice(avail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27bdbcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(value_table, n_episodes, player):\n",
    "    outcomes = {FIRST_PLAYER_WON: 0, SECOND_PLAYER_WON: 0, TIE: 0}\n",
    "    # TODO: same as above training loop, make loop cleaner using cycle to switch through current player or somethign\n",
    "    for i in tqdm(range(n_episodes)):\n",
    "        current = TicTacToeBoard()\n",
    "\n",
    "        while True:\n",
    "            action = None\n",
    "            if player == TicTacToeBoard.FIRST_PLAYER_CELL:\n",
    "                action, _ = sample_policy(value_table=value_table, board=current, player=player, eps=0.0)\n",
    "            else:\n",
    "                action = sample_eval_policy(board=current)\n",
    "            current = current.transition(action)\n",
    "\n",
    "            # TODO: change to use logging\n",
    "            # current.display()\n",
    "\n",
    "            if current.terminated():\n",
    "                break\n",
    "\n",
    "\n",
    "            if player == TicTacToeBoard.SECOND_PLAYER_CELL:\n",
    "                action, _ = sample_policy(value_table=value_table, board=current, player=player, eps=0.0)\n",
    "            else:\n",
    "                action = sample_eval_policy(board=current)\n",
    "            current.transition(action)\n",
    "            # TODO: change to use logging\n",
    "            # current.display()\n",
    "            if current.terminated():\n",
    "                break\n",
    "        \n",
    "        assert current.terminated(), \"should be terminated at end of episode\"\n",
    "        outcomes[FIRST_PLAYER_WON if current.first_player_won() else SECOND_PLAYER_WON if current.second_player_won() else TIE] += 1\n",
    "\n",
    "    return outcomes\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8417eae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:13<00:00, 721.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'first player won': 10000, 'second player won': 0, 'tie': 0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_policy(value_table=value_table, n_episodes=10000, player=TicTacToeBoard.FIRST_PLAYER_CELL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf642387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:21<00:00, 468.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'first player won': 6880, 'second player won': 2678, 'tie': 442}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_policy(value_table=value_table, n_episodes=10000, player=TicTacToeBoard.SECOND_PLAYER_CELL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3983bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(value_table=value_table, path=\"table.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfeefe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_to_ui_format(value_table):\n",
    "    return {\"\".join([\"1\" if c == AGENT_CELL else \"2\" if c == OPPONENT_CELL else \"0\" for c in s]): v for s, v in value_table.items()}\n",
    "\n",
    "save(value_table=serialize_to_ui_format(value_table=value_table), path=\"ui_value_table.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f302e54e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
