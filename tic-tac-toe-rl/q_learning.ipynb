{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6262bf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8567a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tic_tac_toe import Board, new_board, transition, next_marker_to_place, game_state, pretty_format, available_plays, GameState\n",
    "from egocentric import EgocentricBoard, EgocentricMarker, canonicalize, remap_to_egocentric_board\n",
    "from random import random, seed, choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f560696",
   "metadata": {},
   "outputs": [],
   "source": [
    "QTable = dict[str, float]\n",
    "\n",
    "class QLearner:\n",
    "    def __init__(self):\n",
    "        self._canonical_q_table: QTable\n",
    "        self._q_table: QTable\n",
    "\n",
    "    def serialize_state_action(self, state: Board, action: int) -> tuple[str, str]:\n",
    "        \"\"\"Returns the serialized canonical state action pair and the serialized state action pair\"\"\"\n",
    "        def _serialize_state_action(ego_state: EgocentricBoard, action: int):\n",
    "            return \"\".join([str(m) for m in ego_state] + str(action))\n",
    "\n",
    "        ego_state: EgocentricBoard = remap_to_egocentric_board(state)\n",
    "        canonical_state: EgocentricBoard = canonicalize(ego_state)\n",
    "        canonical_state_action: str = _serialize_state_action(canonical_state, action)\n",
    "\n",
    "        # NOTE: we only initialize the canonical q table entry because we\n",
    "        #       will only ever set entries in the regular q table\n",
    "        if canonical_state_action not in self._canonical_q_table:\n",
    "            self._canonical_q_table[canonical_state_action] = 0.0\n",
    "\n",
    "        return canonical_state_action, _serialize_state_action(ego_state, action)\n",
    "\n",
    "    def get_action(self, state_t: Board, epsilon: float = 0.0) -> int:\n",
    "        if random() < epsilon:\n",
    "            # explore\n",
    "            return choice(available_plays(state_t))\n",
    "\n",
    "        # greedy\n",
    "        next_qs: list[tuple[int, float]] = []\n",
    "        for action in available_plays(state_t):\n",
    "            canon_state_action_t, _ = self.serialize_state_action(state=state_t, action=action)\n",
    "            next_qs.append((action, self._canonical_q_table[canon_state_action_t],))\n",
    "\n",
    "        # TODO: consider randomly selecting from ties\n",
    "        return max(next_qs, key = lambda t: t[1])[0]\n",
    "    \n",
    "    def _update_q_tables(self, state: Board, action: int, q: float):\n",
    "        canonical_state_action_t, state_action_t = self.serialize_state_action(state=state, action=action)\n",
    "        self._canonical_q_table[canonical_state_action_t] = q\n",
    "        self._q_table[state_action_t] = q\n",
    "\n",
    "    def update(self, state_t: Board, reward: float, action: int, state_t_next: Board, learning_rate: float, discount_factor: float = 1.0):\n",
    "        canonical_state_action_t, _ = self.serialize_state_action(state=state_t, action=action)\n",
    "        q_t = self._canonical_q_table[canonical_state_action_t]\n",
    "\n",
    "        if game_state(state_t_next) != GameState.INCOMPLETE:\n",
    "            # next state is terminal so all q values at next state will be 0 \n",
    "            self._update_q_tables(\n",
    "                state=state_t,\n",
    "                action=action,\n",
    "                q=q_t + learning_rate * (reward - q_t)\n",
    "            )\n",
    "            return\n",
    "\n",
    "        next_transition_qs: list[float] = []\n",
    "        for action_next in available_plays(state_t_next):\n",
    "            canonical_state_action_t_next, _ = self.serialize_state_action(transition(board=state_t_next, idx=action_next))\n",
    "            next_transition_qs.append(self._canonical_q_table[canonical_state_action_t_next])\n",
    "\n",
    "        # TODO: consider randomly selecting from ties\n",
    "        max_q_next = max(next_transition_qs)\n",
    "\n",
    "        td_error = (\n",
    "            reward +\n",
    "            discount_factor * max_q_next -\n",
    "            q_t\n",
    "        )\n",
    "\n",
    "        self._update_q_tables(state=state_t, action=action, q=q_t + learning_rate * td_error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "306b6d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learner = QLearner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a3fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
