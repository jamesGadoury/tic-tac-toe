{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6262bf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8567a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tic_tac_toe import Board, new_board, transition, next_marker_to_place, game_state, pretty_format, available_plays, GameState, Marker\n",
    "from egocentric import EgocentricBoard, EgocentricMarker, canonicalize, remap_to_egocentric_board\n",
    "from random import random, seed, choice\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f560696",
   "metadata": {},
   "outputs": [],
   "source": [
    "QTable = dict[str, float]\n",
    "\n",
    "class QLearner:\n",
    "    def __init__(self):\n",
    "        self._canonical_q_table: QTable = {}\n",
    "        self._q_table: QTable = {}\n",
    "\n",
    "    def serialize_state_action(self, state: Board, action: int) -> tuple[str, str]:\n",
    "        \"\"\"Returns the serialized canonical state action pair and the serialized state action pair\"\"\"\n",
    "        def _serialize_state_action(ego_state: EgocentricBoard, action: int):\n",
    "            return \"\".join([str(m) for m in ego_state] + [str(action)])\n",
    "\n",
    "        ego_state: EgocentricBoard = remap_to_egocentric_board(state)\n",
    "        canonical_state: EgocentricBoard = canonicalize(ego_state)\n",
    "        canonical_state_action: str = _serialize_state_action(canonical_state, action)\n",
    "\n",
    "        # NOTE: we only initialize the canonical q table entry because we\n",
    "        #       will only ever set entries in the regular q table\n",
    "        if canonical_state_action not in self._canonical_q_table:\n",
    "            self._canonical_q_table[canonical_state_action] = 0.0\n",
    "\n",
    "        return canonical_state_action, _serialize_state_action(ego_state, action)\n",
    "\n",
    "    def get_action(self, state_t: Board, epsilon: float = 0.0) -> int:\n",
    "        if random() < epsilon:\n",
    "            # explore\n",
    "            return choice(available_plays(state_t))\n",
    "\n",
    "        # greedy\n",
    "        next_qs: list[tuple[int, float]] = []\n",
    "        for action in available_plays(state_t):\n",
    "            canon_state_action_t, _ = self.serialize_state_action(state=state_t, action=action)\n",
    "            next_qs.append((action, self._canonical_q_table[canon_state_action_t],))\n",
    "\n",
    "        # TODO: consider randomly selecting from ties\n",
    "        return max(next_qs, key = lambda t: t[1])[0]\n",
    "    \n",
    "    def _update_q_tables(self, state: Board, action: int, q: float):\n",
    "        canonical_state_action_t, state_action_t = self.serialize_state_action(state=state, action=action)\n",
    "        self._canonical_q_table[canonical_state_action_t] = q\n",
    "        self._q_table[state_action_t] = q\n",
    "\n",
    "    def update(self, state_t: Board, reward: float, action: int, state_t_next: Board, learning_rate: float, discount_factor: float = 1.0):\n",
    "        canonical_state_action_t, _ = self.serialize_state_action(state=state_t, action=action)\n",
    "        q_t = self._canonical_q_table[canonical_state_action_t]\n",
    "\n",
    "        if game_state(state_t_next) != GameState.INCOMPLETE:\n",
    "            # next state is terminal so all q values at next state will be 0 \n",
    "            self._update_q_tables(\n",
    "                state=state_t,\n",
    "                action=action,\n",
    "                q=q_t + learning_rate * (reward - q_t)\n",
    "            )\n",
    "            return\n",
    "\n",
    "        next_transition_qs: list[float] = []\n",
    "        for action_next in available_plays(state_t_next):\n",
    "            canonical_state_action_t_next, _ = self.serialize_state_action(state=transition(board=state_t_next, idx=action_next), action=action_next)\n",
    "            next_transition_qs.append(self._canonical_q_table[canonical_state_action_t_next])\n",
    "\n",
    "        # TODO: consider randomly selecting from ties\n",
    "        max_q_next = max(next_transition_qs)\n",
    "\n",
    "        td_error = (\n",
    "            reward +\n",
    "            discount_factor * max_q_next -\n",
    "            q_t\n",
    "        )\n",
    "\n",
    "        self._update_q_tables(state=state_t, action=action, q=q_t + learning_rate * td_error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e8a3fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_from_board_transition(board: Board) -> float:\n",
    "    # NOTE: if next marker is second player, then first player just played\n",
    "    first_player: bool = next_marker_to_place(board) == Marker.SECOND_PLAYER\n",
    "    outcome = game_state(board)\n",
    "    if outcome == GameState.FIRST_PLAYER_WON:\n",
    "        return 1.0 if first_player else -1.0\n",
    "    if outcome == GameState.SECOND_PLAYER_WON:\n",
    "        return -1.0 if first_player else 1.0\n",
    "    return 0.0\n",
    "\n",
    "def rollout_self_play(q_learner: QLearner, epsilon: float, learning_rate: float):\n",
    "    board: Board = new_board()\n",
    "    while True:\n",
    "        action = q_learner.get_action(state_t=board, epsilon=epsilon)\n",
    "        next_board = transition(board, action)\n",
    "        q_learner.update(\n",
    "            state_t=board,\n",
    "            reward=reward_from_board_transition(board=next_board),\n",
    "            action=action,\n",
    "            state_t_next=next_board,\n",
    "            learning_rate=learning_rate\n",
    "        )\n",
    "        board = next_board\n",
    "        if game_state(board) != GameState.INCOMPLETE:\n",
    "            # terminal\n",
    "            break\n",
    "    return game_state(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "306b6d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(42)\n",
    "q_learner = QLearner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9272820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0: GameState.FIRST_PLAYER_WON\n",
      "episode 1: GameState.FIRST_PLAYER_WON\n",
      "episode 2: GameState.FIRST_PLAYER_WON\n",
      "episode 3: GameState.FIRST_PLAYER_WON\n",
      "episode 4: GameState.FIRST_PLAYER_WON\n",
      "episode 5: GameState.FIRST_PLAYER_WON\n",
      "episode 6: GameState.TIED\n",
      "episode 7: GameState.SECOND_PLAYER_WON\n",
      "episode 8: GameState.FIRST_PLAYER_WON\n",
      "episode 9: GameState.FIRST_PLAYER_WON\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    outcome = rollout_self_play(q_learner=q_learner, epsilon=0.1, learning_rate=0.05)\n",
    "    print(f\"episode {i}: {outcome}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2687f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c57538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
