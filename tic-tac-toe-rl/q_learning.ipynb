{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6262bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8567a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tic_tac_toe import Board, new_board, transition, next_marker_to_place, game_state, pretty_format, available_plays, GameState, Marker\n",
    "from egocentric import EgocentricBoard, EgocentricMarker, canonicalize, remap_to_egocentric_board\n",
    "from random import random, seed, choice\n",
    "from tqdm import tqdm\n",
    "\n",
    "from time import time_ns\n",
    "import json\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f560696",
   "metadata": {},
   "outputs": [],
   "source": [
    "QTable = dict[str, float]\n",
    "\n",
    "class QLearner:\n",
    "    def __init__(self):\n",
    "        self._canonical_q_table: QTable = {}\n",
    "        self._q_table: QTable = {}\n",
    "    \n",
    "    @property\n",
    "    def canonical_q_table(self):\n",
    "        return self._canonical_q_table\n",
    "    \n",
    "    @property\n",
    "    def q_table(self):\n",
    "        return self._q_table\n",
    "\n",
    "    def serialize_state_action(self, state: Board, action: int) -> tuple[str, str]:\n",
    "        \"\"\"Returns the serialized canonical state action pair and the serialized state action pair\"\"\"\n",
    "        def _serialize_state_action(ego_state: EgocentricBoard, action: int):\n",
    "            return \"\".join([str(m) for m in ego_state] + [str(action)])\n",
    "\n",
    "        ego_state: EgocentricBoard = remap_to_egocentric_board(state)\n",
    "        canonical_state: EgocentricBoard = canonicalize(ego_state)\n",
    "        canonical_state_action: str = _serialize_state_action(canonical_state, action)\n",
    "\n",
    "        # NOTE: we only initialize the canonical q table entry because we\n",
    "        #       will only ever set entries in the regular q table\n",
    "        if canonical_state_action not in self._canonical_q_table:\n",
    "            self._canonical_q_table[canonical_state_action] = 0.0\n",
    "\n",
    "        return canonical_state_action, _serialize_state_action(ego_state, action)\n",
    "\n",
    "    def get_action(self, state_t: Board, epsilon: float = 0.0) -> int:\n",
    "        if random() < epsilon:\n",
    "            # explore\n",
    "            return choice(available_plays(state_t))\n",
    "\n",
    "        # greedy\n",
    "        next_qs: list[tuple[int, float]] = []\n",
    "        for action in available_plays(state_t):\n",
    "            canon_state_action_t, _ = self.serialize_state_action(state=state_t, action=action)\n",
    "            next_qs.append((action, self._canonical_q_table[canon_state_action_t],))\n",
    "\n",
    "        # TODO: consider randomly selecting from ties\n",
    "        return max(next_qs, key = lambda t: t[1])[0]\n",
    "    \n",
    "    def _update_q_tables(self, state: Board, action: int, q: float):\n",
    "        canonical_state_action_t, state_action_t = self.serialize_state_action(state=state, action=action)\n",
    "        self._canonical_q_table[canonical_state_action_t] = q\n",
    "        self._q_table[state_action_t] = q\n",
    "\n",
    "    def update(self, state_t: Board, reward: float, action: int, state_t_next: Board, learning_rate: float, discount_factor: float = 1.0):\n",
    "        canonical_state_action_t, _ = self.serialize_state_action(state=state_t, action=action)\n",
    "        q_t = self._canonical_q_table[canonical_state_action_t]\n",
    "\n",
    "        if game_state(state_t_next) != GameState.INCOMPLETE:\n",
    "            # next state is terminal so all q values at next state will be 0 \n",
    "            self._update_q_tables(\n",
    "                state=state_t,\n",
    "                action=action,\n",
    "                q=q_t + learning_rate * (reward - q_t)\n",
    "            )\n",
    "            return\n",
    "\n",
    "        next_transition_qs: list[float] = []\n",
    "        for action_next in available_plays(state_t_next):\n",
    "            canonical_state_action_t_next, _ = self.serialize_state_action(state=transition(board=state_t_next, idx=action_next), action=action_next)\n",
    "            next_transition_qs.append(self._canonical_q_table[canonical_state_action_t_next])\n",
    "\n",
    "        # TODO: consider randomly selecting from ties\n",
    "        max_q_next = max(next_transition_qs)\n",
    "\n",
    "        td_error = (\n",
    "            reward +\n",
    "            discount_factor * max_q_next -\n",
    "            q_t\n",
    "        )\n",
    "\n",
    "        self._update_q_tables(state=state_t, action=action, q=q_t + learning_rate * td_error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c92ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(q_learner: QLearner):\n",
    "    dir = Path(\"./outputs\") / f\"{time_ns()}\"\n",
    "    dir.mkdir(parents=True)\n",
    "\n",
    "    canonical_q_table_path: Path = dir / \"canonical_q_table.json\"\n",
    "    print(f\"Saving to {canonical_q_table_path}\")\n",
    "    with open(canonical_q_table_path, \"w\") as f:\n",
    "        json.dump({\"\".join(str(x) for x in k): v for k, v in q_learner.canonical_q_table.items()}, f)\n",
    "\n",
    "    q_table_path: Path = dir / \"q_table.json\"\n",
    "    print(f\"Saving to {q_table_path}\")\n",
    "    with open(q_table_path, \"w\") as f:\n",
    "        json.dump({\"\".join(str(x) for x in k): v for k, v in q_learner.q_table.items()}, f)\n",
    "\n",
    "def load(path):\n",
    "    raw = json.load(open(path, \"r\"))\n",
    "    return {tuple(int(c) for c in k): v for k, v in raw.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6e7b7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learner_play(q_learner: QLearner, state_t: Board, epsilon: float, learning_rate: float) -> Board:\n",
    "    action = q_learner.get_action(state_t=state_t, epsilon=epsilon)\n",
    "    next_board = transition(state_t, action)\n",
    "    q_learner.update(\n",
    "        state_t=state_t,\n",
    "        reward=reward_from_board_transition(board=next_board),\n",
    "        action=action,\n",
    "        state_t_next=next_board,\n",
    "        learning_rate=learning_rate\n",
    "        )\n",
    "    return next_board\n",
    "\n",
    "def random_agent_play(state_t: Board) -> Board:\n",
    "    return transition(board=state_t, idx=choice(available_plays(state_t)))\n",
    "\n",
    "def reward_from_board_transition(board: Board) -> float:\n",
    "    # NOTE: if next marker is second player, then first player just played\n",
    "    first_player: bool = next_marker_to_place(board) == Marker.SECOND_PLAYER\n",
    "    outcome = game_state(board)\n",
    "    if outcome == GameState.FIRST_PLAYER_WON:\n",
    "        return 1.0 if first_player else -1.0\n",
    "    if outcome == GameState.SECOND_PLAYER_WON:\n",
    "        return -1.0 if first_player else 1.0\n",
    "    return 0.0\n",
    "\n",
    "def rollout_self_play(q_learner: QLearner, epsilon: float, learning_rate: float):\n",
    "    board: Board = new_board()\n",
    "    while True:\n",
    "        board = q_learner_play(\n",
    "            q_learner=q_learner,\n",
    "            state_t=board,\n",
    "            epsilon=epsilon,\n",
    "            learning_rate=learning_rate\n",
    "        )\n",
    "        if game_state(board) != GameState.INCOMPLETE:\n",
    "            # terminal\n",
    "            break\n",
    "    return game_state(board)\n",
    "\n",
    "def rollout_random_opponent(\n",
    "    q_learner: QLearner,\n",
    "    epsilon: float,\n",
    "    learning_rate: float,\n",
    "    agent_is_first_player: bool\n",
    "):\n",
    "    board: Board = new_board()\n",
    "    while True:\n",
    "        if next_marker_to_place(board) == Marker.FIRST_PLAYER:\n",
    "            board = (\n",
    "                q_learner_play(\n",
    "                    q_learner=q_learner,\n",
    "                    state_t=board,\n",
    "                    epsilon=epsilon,\n",
    "                    learning_rate=learning_rate\n",
    "                ) if agent_is_first_player else random_agent_play(state_t=board)\n",
    "            )\n",
    "        else:\n",
    "            board = (\n",
    "                q_learner_play(\n",
    "                    q_learner=q_learner,\n",
    "                    state_t=board,\n",
    "                    epsilon=epsilon,\n",
    "                    learning_rate=learning_rate\n",
    "                ) if not agent_is_first_player else random_agent_play(state_t=board)\n",
    "            )\n",
    "        if game_state(board) != GameState.INCOMPLETE:\n",
    "            # terminal\n",
    "            break\n",
    "    return game_state(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "306b6d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(42)\n",
    "q_learner = QLearner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9272820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000000/5000000 [15:28<00:00, 5383.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<GameState.FIRST_PLAYER_WON: 1>: 3375366, <GameState.SECOND_PLAYER_WON: 2>: 1212047, <GameState.TIED: 3>: 412587}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outcomes = {GameState.FIRST_PLAYER_WON: 0, GameState.SECOND_PLAYER_WON: 0, GameState.TIED: 0}\n",
    "\n",
    "for _ in tqdm(range(5000000)):\n",
    "    outcome = rollout_self_play(q_learner=q_learner, epsilon=0.1, learning_rate=0.05)\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "print(outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c2687f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000000/5000000 [09:00<00:00, 9247.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<GameState.FIRST_PLAYER_WON: 1>: 3837175, <GameState.SECOND_PLAYER_WON: 2>: 517016, <GameState.TIED: 3>: 645809}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# q learner is first player\n",
    "\n",
    "outcomes = {GameState.FIRST_PLAYER_WON: 0, GameState.SECOND_PLAYER_WON: 0, GameState.TIED: 0}\n",
    "\n",
    "for _ in tqdm(range(5000000)):\n",
    "    outcome = rollout_random_opponent(q_learner=q_learner, epsilon=0.1, learning_rate=0.05, agent_is_first_player=True)\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "print(outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23c57538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000000/5000000 [07:33<00:00, 11013.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<GameState.FIRST_PLAYER_WON: 1>: 1190328, <GameState.SECOND_PLAYER_WON: 2>: 2827542, <GameState.TIED: 3>: 982130}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# q learner is second player\n",
    "\n",
    "outcomes = {GameState.FIRST_PLAYER_WON: 0, GameState.SECOND_PLAYER_WON: 0, GameState.TIED: 0}\n",
    "\n",
    "for _ in tqdm(range(5000000)):\n",
    "    outcome = rollout_random_opponent(q_learner=q_learner, epsilon=0.1, learning_rate=0.05, agent_is_first_player=False)\n",
    "    outcomes[outcome] += 1\n",
    "\n",
    "print(outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f62c31c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to outputs/1753565858454676581/canonical_q_table.json\n",
      "Saving to outputs/1753565858454676581/q_table.json\n"
     ]
    }
   ],
   "source": [
    "save(q_learner=q_learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d27f677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
